# What Palantir Sees

*The tech company’s C.T.O. on surveillance, A.I. and the future of war*

## Transcript

What Palantir Sees  
The tech company’s C.T.O. on surveillance, A.I. and the future of war  

Your company is named for seeing stones. Used to view things at a very long distance in J. R. R. Tolkien’s Lord of the Rings. Also worth noting that in the course of Tolkien’s story, they fall into the wrong hands and are used for evil as well. The naming choice was intentional. It’s a built-in warning and a reminder to us. What does the most mysterious and paranoia-inducing company in all of Silicon Valley actually do? How did the tech industry learn to love the military industrial complex? Is artificial intelligence about to revolutionize warfare? Help the United States win an arms race with China? Or place us all under digital surveillance? My guest today was recently commissioned as a Lieutenant Colonel in the U.S. Army reserves. And in his day job. He’s the chief technology officer of Palantir, a company whose relationship to the U.S. government is increasingly profitable and immensely controversial. Shyam Sankar, welcome to “Interesting Times.”  

Great to be here. Thanks for having me, Ross.  

So we’re going to talk about a lot of things. We’re going to talk about your biography and background, how you came to be an officer in the U.S. military, the future of technology and warfare. But we have to start with a very, very simple question. What is it that Palantir does?  

Great question. Thank you. I spent a long time crafting it. Most important question.  

Yeah let’s start just at a foundational level. We’re a software company. And we build software that allows you to manage your data to make better decisions. And I think that’s best understood through an example. So I spent a lot of my time helping companies manufacture things really the reindustrialization of America. So if you’re a manufacturer, you have a system called a PLM system, a Product Lifecycle Management system, that you use to design your product. You have another system, a manufacturing execution system that you use to manage the manufacturing, the actual production of it on the assembly line. You have another system called an ERP system for inventory management, supply chain management, and yet another system for managing sales orders, a customer relationship management system. What we do is we build software that allows you to bring the data from those systems together, so that you can manage the process holistically. Now half of what Palantir does is commercial. We work in 50 different industries, from energy and mining to pharmaceuticals and insurance. In the commercial world, you’re optimizing the value chain. You have a series of decisions that you’re making from the hand of your supplier to the hand of your customer. And of course, you can generalize it to the military, which we’re very well known for what we do there. You can think about that as you’re optimizing the kill chain from sensor to shooter. They call it doctrinally, but it’s the same thing, which is, how do I find the enemy targets. How do I decide which targets I want to prosecute based on maybe the inventory I have on hand the effect it might have to the enemy. How do I manage my personnel readiness. How do I manage my equipment readiness. All of these things need to work or it doesn’t work.  

So your CEO, Alex Karp, did an interview with my colleague Maureen Dowd a little while ago, and he had this extremely resonant phrase. He said, Palantir is in the business of the finding of hidden things. But a lot of the narrative about Palantir assumes that means you go to work for a government or a corporation, and then you go out and find those things in the external world. But in your description, it is you’re finding things that institutions already have access to but don’t know or don’t understand.  

Yeah the cynical way to think about Palantir is it took something as sexy as James Bond to motivate engineers to work on a problem as boring as data integration, but that’s actually the core of what generates the value that if you look at any of these institutions, they have so many silos of data, they all have a specific purpose, but the greater truth is lost in the seams between these systems. And historically, we’ve just relied on humans connecting the dots in their heads. And so we build Iron Man suits for those humans to be able to ask questions of the data, usually with a normative view of getting to a decision. So it’s not insights for insights sake, but it’s actually what am I trying to do. How do I optimize the operation of the business.  

And so obviously, from the public’s point of view, the reason Palantir is a big figure in national politics and national debates is that half of your work is done for the military, the government and so on. And that’s not just a business choice, right? There is a kind of values based theory that Palantir has of why you do that kind of work. Can you talk about that?  

Yeah, and I would even just state it even more strongly. We started in the government. We started really as a small group of folks who wanted to work on problems in national security. And if you rewind and go back to this immediate period, post 9/11, the kind of political discourse was very much something like, what’s more important, privacy or security? I don’t as a bunch of engineers, we thought that was a silly question. They both seem really important. Why wouldn’t we be trying to have more of both? You have to actually build technologies that are cognizant of both ends of these things and are pushing out the frontier. And we’re going to come back to the privacy question a bit more. But in the case of government, you have silos of information that the government has access to. But some parts of the government are not supposed to look at or use or deploy for reasons having to do with constitutional rights and privacy. But in the end, you are not responsible for the choices that companies or the Department of Defense make about when to access something and when not to. You create a structure that gives them opportunities to say why you can’t access this, or this department can’t access that, but they’re in the end, they’re running, they’re responsible. They’re responsible.  

O.K. and I didn’t really address your question of the moral view we have, which is why we started the company, that the West is a force for good. We believe in strengthening the legitimacy of these institutions. We should have been able to connect the dots and prevent 9/11. So what is the role of us as technologists in this country. What is our obligation to America. How can we be helping. And we believe in leaning in solving these problems that our warfighters should have the best software. And here, it’s worth noting that your company is named for seeing stones. Used to few things at a very long distance in JRR Tolkien’s Lord of the Rings and the Palantir these seeing stones were used by the noble kingdoms of the West. In that story are also worth noting that in the course of Tolkien’s story, they fall into the wrong hands and are used for evil as well. The naming choice was intentional. It’s a built in Warning and reminder to us that you’re basically building something that is potentially powerful, and you have an obligation on how it’s used, and you have to be thoughtful of this stuff that. My broad critique societally would be there’s what to build and how to build it. The Valley is obsessed with how to build things like engineering is a craft, and very little thought is given to what should we build. What are the valuable things for society. What is the greater second order consequence of what we’re doing here. So I’d like to get both of these things right. You have to be right about how you’re going to build these things. World class software. And you have to build the right thing that matters for the nation. But any right thing, as you said in the Warning, is going to be powerful enough to be misused, potentially.  

Yeah, right. By the Saruman’s, if not the Sorin’s of the 21st century.  

So now I want to see if you can walk me through two examples, one foreign and one domestic of the zones where Palantir software is being used by the government. And I’ll see how concrete I can get you to make it, because I know that we’re talking about areas where there’s confidentiality and national security. And so on. So let’s start. You talked before about the kill chain. What is the kill chain.  

The kill chain is the sequence of decisions. So this is a doctrinal military term of how you go from sensor to shooter. It’s the targeting process. So how do I identify where the enemy is. How do I confirm positive ID go through the rules of engagement, get the Jag sign off that this is a legal target. How do I then send that to the gun line to prosecute the target. Having prosecuted the target, how do I then do a battle damage assessment and understand did I hit the target. And then cycle back around to what do I do next.  

So let’s again to make it even more concrete. So US Special forces, let’s say are operating in Central Asia or the Horn of Africa or something, right. And they are faced with a particular mission. And what happens that Palantir is involved in.  

They have a mission. They’re trying to take out a member of a terrorist group, let’s say. Or we could use an example like the Houthis so you could say that there’s some starting point of intelligence that it could be from overhead imagery, it could be from any number of sensors that are out there that there’s something at this location. Now then how do I primarily satellite information or just a whole range terrestrial sensor, satellite information. So space based terrestrial, those are the two opportunities, really. You’re never going to just prosecute off of one single source of intelligence. So O.K. Can I corroborate this with other sources of intelligence. How do I get a positive ID that this is not a decoy. And maybe I’m not misattributing what it is that I’m looking at here. Then there’s a prioritization process. Is this worth shooting. Is this a threat. What is the effect of taking this capability off the battle space. Does it provide safety. Does it provide more room to maneuver. Or is it actually kind of irrelevant. Let’s assume we decide it’s important to take out of the battle space. Maybe they’re going to fire a missile at a destroyer. Then it’s O.K. Well, which gun. From a Destroyer. No, they were going to fire. Oh, sorry. Yeah, the Houthi are going to fire a missile at our destroyer. So we’re trying to preempt. Yeah O.K, so then we go through the process, there’s legal sign off, then you get to the point of like, O.K, well, how do I want to interdict this. Am I going to use a $2 million missile. Am I going. What are my options for doing this. How long will it take. What are the consequences of logistics. What’s the time and flight to get there. Can I actually action this. Then there’s sending that to the gunline or the weapon system that will do it. And then there’s scheduling the collection. Maybe you’re going to use overhead imagery, or you’re going to fly a drone with a camera to see having shot it. Did we eliminate it or not. So that cycle is really that’s the kill chain. And the whole point of this is applying technology to be faster than your adversary at going through that. If you’re faster than your adversary at going through that, assuming you’re fighting, you’re going to win. Assuming you’re not fighting, you’re going to have maximal deterrence.  

So we here is the US military. Yes it’s not Palantir. That’s right. You are not personally running the kill chain. But so who is actually managing the process. Is it. It’s just the combatant Commanders the green suiters and blue suiters, as we call it, the military itself.  

So the soldier on the ground is therefore is downstream of the information that Palantir software is collating and spitting out for the people giving commands to him or her.  

That’s right. The software extends to enable collaboration. So I think part of the value is want a single pane of data. We’re all seeing the same thing, but you have multiple panes of glass. So the soldier on the ground might be looking at on their mobile phone. And you might have in the situation room, a big command center view of the whole battle space. But what you want to know is that we’re all looking at the same thing, even if we’re looking at it through different applications or mechanisms relative to where we sit.  

So this would apply basically across any area where the US is doing military intervention. So what you’re describing presumably applies to elements of our attack against the Iranian nuclear program or it would apply to the politically highly controversial attacks in the Caribbean or the Pacific that the Trump administration has been conducting. Those are the overseas zones where what you’re doing is integrated into what the US military is doing.  

Yeah our software is deployed ubiquitously across the military. I mean, I think there’s actually even a bigger question there where the military’s a massive enterprise. And I think one of the most important missing strategic assets for us is an integrated view from the factory floor to the foxhole. People spend a lot of time thinking about the sharpest end of the kill chain, how do we intervene for understandable reasons. But if you think about the complexity of the problem, you start realizing, well, we spend a large amount of money building weapons. How do we we’re building the right weapons. How do we know the consequences they’re going to have in the foxhole three years from now. When you deploy DDGS out, a destroyer out. All these ships need maintenance at some point. So if you’re extending the deployment, you’re deferring the maintenance. How do you trade off present readiness against future readiness. These are the sorts of questions that I think are much more strategic that are hard to answer, but are determinative on delivering deterrence.  

So obviously, anything the US does that kills people overseas is controversial. But probably the most controversial thing that Palantir software is being used for right now, I would say, is for immigration enforcement with ICE and other organizations. Can you talk a little bit about how Palantir software is integrated there.  

Yeah, certainly. Well, first, I think it’s a lot of the controversy moves with the politics. We first started working with ICE when Jaime Zapata, an ICE agent, was assassinated in Mexico in 2011. I believe it’s either 2010 or 2011 under the Obama administration. And we’ve worked with ICE from then on until the present moment. The work is structurally very similar. It’s like there’s information that DHS has access to that are spread out across the component agencies that humans are trying to look up. They’re literally doing searches in all these systems. So how do we enable them to actually see the whole picture and then use that for resource allocation. So where do we have threats. Where should we be allocating people. How do we prosecute removal operations. That’s in many ways exactly what we’re doing.  

So a couple things. So number one, I think Palantir, you have a $30 million contract to build a platform called immigration OS. So what kind of data that’s available to enforcement and removal operations. So encounters at the border right. Asylum applications or the lack of one applications for benefits. So a lot of this data really relates to non non-permanent resident non-us non-u.s. citizens who are interacting with various facets of DHS as they come here. So it’s data on specific encounters, encounters they have with Border Patrol, criminal justice system, and so on. But does it extend to things like last known address. Is that the kind of data in when it comes. So what was the address they reported they were in the border encounter. What was the address they reported when they applied for benefits. So then that becomes the kind of data that if ice is doing a raid, they end up using to say, O.K, we have this person here. And so on. So right. So it is effectuating raids through that mechanism. To what extent does it go beyond that. Is there integration with local law enforcement, for instance, with any of this stuff. But I mean, what is the I think the real question, just zooming out a little bit, we’re working with agencies that have lawful authorities. So what data are they collecting. That’s the data we’re helping them integrate. So if there’s a lawful authority for them to collect the data, I don’t think local law enforcement is part of this. I’m not saying I have an objection to them being part of this. I’m just saying that. No, I’m just trying to get into I mean, I think the core fear that people have about well, there’s two layers of fears that I think people have with this kind of domestic data collection that are distinct from people who just oppose deportation. One layer, which as far as I can tell is overstated or wrong, is that Palantir is again, going out and devising, we have no data. You’re not devising, but you’re not devising systems to acquire new forms of data for the government. But you are creating systems where the government has access to data. And I think this is important on a scale that no government has ever had access to before. One of the unique features of 21st century America, digital age America is that, it’s just so much easier to track and observe people in all kinds of different ways. And corporate America does that even more than the government does, right. But I think so. I don’t think that’s the fundamental inflection, though. I think it’s really that the number of decisions you’re trying to make are growing much more quickly than your headcount is growing. So let’s just say in the ice case, you would have had roughly on the order of 20 systems that you would have to manually go search across. How many times if you’re trying to find the address, the address of someone who you want to deport. Do you think you’re going through all 20 systems. Maybe you stop at 10, maybe you forgot to search some systems. So the entropy is just that you’re not actually able to do the work. You’re fighting, you’re tooling the whole time. And this leads to the lack of legitimacy of the institution. It’s like you’re not able to do the thing that you’re actually supposed to do. Of course, the complexity of this is growing as you’re asked to do more and more. It’s like maybe I was doing 10 things before and I could do with the people I had. Now I have to do 1,000 things. That means I clearly don’t get to all 1,000 things. And the things I get to I’m not doing them well. It’s like, O.K, well, why are we using the human to search 20 different systems. That’s clearly a problem that technology can do. How do we use the human to do things that are uniquely human judge that require human judgment, right.  

I guess I’m just saying that I think for at least some people and maybe I’m among them, there’s a kind of relief in the idea that all of this data has multiplied so quickly that people can’t that authorities can’t quite keep up with it. So if we walk outside, if we walk outside of this room and enter, Midtown Manhattan, we are under constant surveillance now. It’s not all it’s not government surveillance. But there’s a kind of relaxation that you can feel where you’re like, O.K, but all of this surveillance is distributed across so many different public and private entities. And unless I am literally a terrorist, the odds that people are going to be constantly watching and scrutinizing me are very low. But then the fear becomes, well, if we have this incredible way to make it all. More and more and more efficient, then maybe privacy does start to disappear, I don’t what do you make. What do you make of that.  

There’s two thoughts there. One is well, are you saying that you feel safer because the institutions that are supposed to protect you are structurally incompetent. And that’s the part where I feel like the answer might be Yes sometimes. Yeah And then a consequence of that, which, I think a democracy can decide a consequence of that, is that they also can’t do their job. They can’t protect you from the things that they’re supposed to protect you from. So I’d offer another solution to this, which is they should be really good at doing what they’re doing. And we should have a strong ability to oversee that. They’re not doing things that they’re not supposed to be doing. That’s exactly what we designed Palantir to do. So this wasn’t our system. But I think it’s an illustrative example, because it was high profile at the time. If you go back in time, there were government employees who looked up the passports of Hillary Clinton and Barack Obama, and they got caught. How did they get caught. The system they’re using generates audit logs. People were reviewing the audit log. And did you have a permissible use to look this up. The answer was clearly no. And they were quickly caught and terminated. You and so you have to both help the people who are protecting us, and you have to empower the people who are watching the watchers so that they also have asymmetrically strong technology to ensure that there are no abuses.  

So who is watching the watchers? So let’s we can stick with immigration enforcement, right. So presumably there’s data and information that is protected by privacy that ice is not supposed to be accessing. So the first line of defense is on the front end. Like, do you even have authority to have this data. So the lawyers are on the front end. Are you have data use agreements that they have with their own interagency data use agreements, the authorities, they collected the data to begin with. And then the second line of defense is after they have the data is, are you using it for a permitted use. And that’s usually I don’t know for every agency this is look, I’m a builder. So you consult a policy person. But something like the Office of the Inspector General, other law enforcement agencies that are responsible for this, sometimes, depending on the context, it can actually be counterintelligence. It’s the same infrastructure that you would use to make sure that people aren’t abusing the data is how you would also make sure that don’t have a spy in your organization. In the end, though, it is up to the people running those institutions to decide whether to basically track abuses at all. And it might be at different levels in different agencies, but presumably in the Department of Homeland Security, Kristi Noem would have the unique authority to track or not track abuses to some degree.  

I mean, yeah, I think that’s a little bit of a nihilistic view. I mean, statutory requirements on this. So I think there’s a fundamental question. If I understand the perspective, I don’t agree with it, that, hey, we can’t trust these institutions at all. It’s like there are rules. They follow the rules. They’re also humans. They’re infallible. But I don’t think people are like Willy nilly deciding whether they want to do this or not. No, I don’t think so either. I’m just trying to frame what I think are the most commonplace criticisms and critiques and worries about how this technology is used in the context of the second Trump administration. I think from the point of view of someone who is a skeptic of Palantir might say, well, in the end, you are putting a lot of power into the hands of people that this person who thinks Donald Trump is a threat to the Republic, right. Doesn’t trust. So there is just a way in which you independent of what you think about Trump himself. When Palantir goes to work for a government, US government or any other government, you are putting yourself in the position of trusting that government with this very impressive technology that you’ve built.  

Yeah, I think that’s right. You have to pick your customers. So can I just ask then about that process. Because I think it’s interesting. How do you calibrate that kind of decision making. So just recently the UK, United Kingdom rolled out a plan to have a new digital ID, and I believe the head of Palantir in the UK said, we don’t think there’s enough privacy safeguards here and we’re not going to cooperate with the government on this. So that’s like an exact example of making this kind of practical but also moral decision making. What kind of thinking goes into a decision like that.  

Well, I’m probably not the best person to comment as the CTO, but I think I’d have to go back to the quote. I think what Louis Mosley said was that this is an issue for the ballot box. Louis Mosley is the head of Palantir in the UK. That’s right. So look, there’s a lot of discursive interaction, both internally at Palantir. There’s an obvious threshold question of is this a legal use. Is this a government that you trust and work with. And then beyond that, it’s really a question of are the authorities there. What’s the potential for misuse. Are the protections there in our software against those potential abuses. And is the work actually going to lead to something meaningful.  

See that’s really interesting though right that he said it’s a matter for the ballot box. And again I know you’re not running that decision. But it’s still as an outsider to the work your company does, it seems like he or the larger corporate enterprise made a judgment that basically a political judgment that the digital ID issue had not been adequately litigated by British democracy for you to cooperate with it. Isn’t that kind of a fascinating. It’s a fascinating decision. I mean should talk to Louis about that. I would give you the other side of that, which is, I think a lot of the policies, a lot of the things that people are struggling with right now, I think in the US were voted on at the ballot box. What ICE is doing. Was voted on at the ballot box. So right. No So you so Palantir again, not that you speak for everyone at Palantir, but Palantir is more comfortable with mass deportations in the context of a world where Donald Trump campaigned on mass deportations and won, then it might be in a world where he had not mentioned deportations at all, and then asked Palantir to design a mass deportation abetting software. What I’m saying is the people voted on this. That seems like a functioning democracy there. And of course, there’s going to continue to be discursive interaction and disagreement. That’s our. That’s the beauty of our political process. But it’s an important distinction.  

Yeah how do you think about that with foreign clients. Like obviously Palantir does work for the state of Israel. That is, again, obviously a source of controversy. But so you’re making a judgment that Israel is an appropriate a morally appropriate partner Yes Do you do work with dictators, with non-democratic governments. We do work with the US and our allies. It really the nature of the work depends particularly, I think, in the coalitions that are fighting terrorism, the coalitions that are pushing back on the great power competition that we’re in with the big four. But so would you be if the Kingdom of Saudi Arabia called you up and said, we just want to build a digital ID system, you would be less likely to do that work because they’re not a Democratic government. Would that be fair. I would leave that up to Alex to figure out. It depends on their uses. I’m not sure that’s exactly fair. I mean, is it leading to a more efficient electronic government. What’s the purpose of it is the question. There’s a difference of which agency is doing it. And what is their mandate. What data do they have. How are they trying to what are the workflows they’re trying to enable. But from the beginning, right. Just again, in the name from Lord of the Rings. You have this idea that you are of you’re building a technology that in the story the name originates from, is used for good and for evil. I’m just interested in the extent to which that kind of judgment is more than most companies, woven into the kind of governmental work that you’re willing to do.  

I mean, we pick our partners very carefully. We want to work on work that’s important to the West, and that includes the commercial work we do in industrializing the country.  

So let’s use that as a way to pivot to your own biography for a minute. Back in June, you were one of four tech executives to be commissioned as a Lieutenant Colonel in the Army Reserves. Why did you do this.  

For a lot of reasons. The primary one is. Is personal. Like, what example am I setting for my children. What did I learn from my father. So you have to go back to how I came to America. My family fled violence in Nigeria when I was a child. Our dog was decapitated. They pistol whipped my father. They threatened my mother. And I was a young child. So I don’t really have direct memories of this, but obviously it was very traumatic for my parents. And we left all of our earthly possessions behind and restarted life in Orlando. And dad was he was this person who was wildly successful in his seconds and never really successful again after that. After he came to the US, it wasn’t the classic immigrant story. It was a guy who tried to start something, went bankrupt life was always kicking him in the teeth. But he was always so grateful to be here. Life, liberty, and possibility. He would remind us of the counterfactual like. But for the grace of this nation, you’d be dead in a ditch in Lagos. And so we always grew up with that deep understanding of this. I don’t know how useful. I really could have been to the military when I was 23, but I think at 43 20 years of building Palantir, really learning all the mistakes I made, compounding into actual lessons. I think I can be much more useful. And the opportunity to serve there were times when my dad really wished I would go to a service Academy, but I think what really catalyzed me to push on this is my observation in Israel after October 7. Israel is an incredibly technical country, bountiful resources of technologists. The IDF is incredibly skilled when on October 8, they mobilized 300,000 reservists. By definition, all of them were prior service, conscription based country. When the reservists. And what was their day job. Well, they were mostly technologists now with 20 years of experience, roughly when they came back in, they were horrified at the state of technology, which is actually an implicit self-critique. It’s a version of what I just said, well, yeah, at 20, at 20, I knew how to code, but I didn’t know what I was doing. And Wow, I would just build these things differently. I would architect them differently. The IDF got more modernization done in the four months after October 7 than in the 10 years that I’d worked with them prior. And well as bountiful as technologists are in Israel, we have the world’s best technologists in America. We have uniquely talented people just like that today are outside of defending the nation. We have made voluntary civil military fusion impossible when our adversaries make it a requirement. And so it’s great to have some venue to have America’s greatest technologists actually have some opportunity to give back and contribute. And so the first the four of us are the first Salvo at that. But I think there’s an opportunity to really scale that.  

So what are you actually doing during that time.  

Each of us have different jobs that align with our skills. Here my focus is really on the workforce, workforce development. I have a personal interest and theory in identifying the heretics that are in the Department. There’s so many talented green suiters how do we find them and make sure that they’re in a role that’s big enough for their actual skills. Rank often gets in the way of really getting the right person in the right role. And so that’s one piece of it. The other piece of it is just actually helping them with how they plan. So if you say, look, the army of 2030 needs to look like this, and I need this many people for all these sorts of skills, we have these emerging roles. How do we actually then what’s the software infrastructure that allows us to decompose that into. Well, how many people do I need to recruit now. How am I doing against that goal. What’s my pipeline for training and developing them. That’s closer to my job.  

Jar who are the other three who were commissioned with you.  

We have the CTO of Meta, Andrew Bosworth. Boss Bob McGrew, who was the former chief research officer at OpenAI. And before that he was at Palantir for 10 years. And Kevin wheal, who’s the chief product officer of OpenAI.  

But would you say all of you in different ways are involved in bureaucratic modernization or like.  

Yeah that’s fair. Do you think there’s any potential tension between your work, your obligations, basically your responsibilities at Palantir and your responsibilities to the US military.  

Well, I think hopefully everything I’ve learned commercially, working with thousands of commercial customers, can be repurposed to actually more efficiently deliver solutions here. There’s a reason I’m focused on work that doesn’t overlap with anything else that Palantir does. So there’s no appearance of conflict. But the army has lots of reservists, and those reservists have day jobs, and the Army has a whole process for managing conflicts and assigning what it is that you can work on based on what is your day job.  

O.K, so there’s someone above you effectively in the military chain of command who’s deciding if you’re crossing lines. Do you see any tension between your own personal narrative and family history. As an immigrant to the US and the fact that Palantir is working to effectuate mass deportations. I guess more generally, what is your view of immigration policy based on that biography.  

Well, my own view is that assimilation is important. Like we came here legally, we adopted, we believe in America. That immigration has made this country strong. But legal immigration, there are rules to follow. And it’s even true if you just Zoom out. It’s like, how will you maintain the society where broad parts of the American people feel gaslit by the process. It is actually the lack of enforcement breeds nihilism and. That’s not tenable, right. I mean, I agree with that. There is also, though, I think clearly a conflict or a tension in right of center politics right now between people who take your view and basically say immigration is good for America, but the system we’ve had is a disaster. And we need to restore order. And people who say we just have too many immigrants, period. And you could say South Asian immigrants in particular I don’t do you see yourself as effectively trying to make immigration policy more credible for the purpose of allowing continued immigration.  

Well, I’ll let that be decided by at the ballot box. I think what I view is the latter thing that you pointed out is a reaction to having no enforcement of the former. So it’s easy to go to and say that. But when you feel like you’re losing jobs, when you feel like the society is being torn apart, when you feel like no one’s being responsive. This is what happens. And this is why I really believe in getting the organizations to be legitimate and function and work. Otherwise, what is the basis for making policy. It’s really tenuous right now.  

So that seems like an attempt to have a kind of civic nationalist vision from the perspective of corporate America. And I think I’m just speaking from my individual perspective here. I know you’re not offering Palantir perspective on immigration policy. But you are, you’re a corporate I won’t say a corporate Titan, but you are a leader of corporate America. And so it’s notable to have this kind of civic nationalism expressed in part because this was not, I think, the dominant mindset in Silicon Valley in the 1990s and 2000. For an extended period of time. I think the tech industry was very much defined by not a hostility to America, but a kind of post-nationalist were citizens of the world, were serving global culture, and certainly not just true of Silicon Valley. I think it was true of a lot of elite American institutions in ways that contributed to the populist rebellion in the end, and the idea of tech executives serving in the military, I think, is a very, very concrete shift away from that older perspective. And I don’t think it’s just Palantir where you see that shift. There’s hundreds of tech and AI startups that are focused on military technology. And we got direct commission, the four of us, between the four of us, maybe 1,000 people in the Valley reached out saying, one, this is amazing. And two, how can I get involved. But what do you think is driving this change in attitude overall. Because it seems notable I lived through September 11, there was some kind of shift after September 11, but it seemed to dissolve pretty quickly. What do you think is happening that makes this kind of mentality more common in Silicon Valley.  

I don’t want to be reductionist about it, but I think you can’t discount the effect of the invasion of Ukraine. It was a moment where people kind of realized all these things they took for granted, actually just don’t happen on their own. And that actually someone can just decide to roll a bunch of tanks across a border and try to change a fundamental reality in the world. And you can actually dissuade that without hard power, that perhaps there is still evil in the world and that evil is not us. You could characterize a lot of the malaise of the last 20 years is kind of feeling like, Oh, we screwed up in Iraq or Afghanistan. And maybe we’re the problem. And this element of self-loathing that I think Putin’s invasion brought a lot of clarity to and a recognition that these things have to be done well.  

That’s really interesting because again, as with immigration, it seems like there are two very different reactions to the invasion that are present in American politics on the center right in the Trump administration. And one sees it as kind of a continuation of the Iraq, Afghanistan story, where it’s look, here again, is another foreign war that Americans are unwisely enmeshed in. And in fact, it’s yet another case study in how we need to effectively retrench and rebuild. But retrench. But you’re offering. What is, in the end, I think, a much more hawkish view of the lessons that the US should take from.  

Well, I’m offering a view of what I think is galvanized Silicon Valley. And certainly I think different pockets of America have different reactions to this. I think, though, there’s a broader narrative that unfortunately unfolds over many decades that people have lost the plot on. And you could ask arguably for the last 10 years, we’ve lost deterrence as a nation. You look at the Crimea in 2014, the militarisation of the Spratly Islands by the Chinese in 2015, the breakout capability of Iran on the bomb, and then a pogrom happening in Israel two years ago. So peace is not there right now. That doesn’t mean I’m saying we need to go out and have wars all over the place. But peace comes from deterrence. And we’ve lost deterrence. Now, there’s a national question of what should we do about that. My argument from a builders perspective, is that a big part of why we’ve lost deterrence is the excesses of having won the Cold War, or I think more accurately, it should be framed as the Soviets lost the Cold War and being the sole superpower for some period of time. If you really look at what made the industrial base work in World War 2 and the early Cold War, it wasn’t a defense industrial base. It was an American industrial base. We have kind of whitewashed history to forget that Chrysler built minivans and missiles, Ford built satellites until 1990. General Mills, the cereal company, used to build torpedoes. That American innovative spirit. Now, the conventional wisdom of this story is that we lost competition in the industrial base. This consolidation just led to less players, and they became less good at providing it, maybe a little fat and happy. I don’t think that’s right. I think what actually happened is that you drove out the crazies, and the wild engineering spirit moved out to other parts of our economy, parts of our economy that became disconnected from serving the nation. And that’s what’s changed in this present moment, whether it’s the invasion of Ukraine, you could say SpaceX, Palantir and/or led it before then. Interested group of founders, technologists who want to build in the National interest and who have heterodox ideas. You need the heretics. The heretics end up being your heroes. Even the formation of the Air Force was an act of heresy. You had Billy Mitchell, who at the time was part of the army, and he really pissed off the Navy by showing that you could drop a bomb from an airplane instead of using a boat to do it. He was court martialed. He died in disgrace posthumously. They created the Air Force, so he didn’t even get to see his heresy become heroism. But he’s the founder of the Air Force. And I think more than ever in this time, we need our heretics.  

So is that the fundamental tension in a way, between the new players in the Silicon Valley part of the defense industry and the old guard that you are offering yourselves as potential outside builders for things that the existing services prefer to build themselves. What is that a tension. What are the tension points.  

Five years ago that was really a tension point. Famously Palantir had to sue the army not once but twice to get the right to compete. And the army wants to build its own. So this isn’t just about ships. The army wants to create its own proprietary software. Humans want to create their own things. So it’s not like, hey, the army’s bad or the service is bad. This is a function of human nature. Our greatest competition in the commercial world is the same thing. It is that institutions desire to build their own solution, rather than having this foreign object come by that. Not invented here. Syndrome people call it. And what is the clarifying way of thinking about this.  

Well existential threat. So Yeah. Having won the Cold War, facing no threats, you’re going to go down a long path of indulging in this the largesse of building exactly what you want and how you want to do it. There’s no back pressure. There’s no threat that’s going to align you to developing the right thing at the right speed. What’s going to empower the heretics there. And I think the world has changed anyway. So that’s why I say in the present, five years ago that was the problem. Now it’s very different that the amount of heterodox thinking that’s being absorbed, the way I often frame it is the Department of Defense is a monopsony. Department of War is a monopsony like people are very familiar with Monopoly, where you have one seller of a thing, but a monopsony is when you have one buyer of a thing, and a lot of the dysfunction is that’s the root cause that you have a single buyer. You can think about the great monopsony of the 1990s, Walmart and what was their tagline, everyday low prices. You can remember the ads where they would roll back the prices. So their strategy was effectively we have the distribution we need to squeeze our suppliers and control price. They didn’t see Amazon coming. This is the problem for the monopsony. You don’t have enough competition to realize what’s actually happening in the marketplace. And competition in this area just takes the form of military conflict. In the end. Or at the very least, the strong threat of military conflict.  

So talk about from the point of view of the heretics. But as much as you specific to Palantir, what are the changes that would prevent the US from losing the next war.  

Well, I think that AI and software is a wonderful asymmetric advantage of the US. I mean, there are no Indian or Chinese enterprise software companies that are on the world stage. It is a unique strength that we tend to underestimate here. I is an American phenomenon. Deep tech would not exist if they had not stolen the model from OpenAI through distillation. Yes, they had clever optimizations, but the fundamentals of it this is a phenomenon that’s happening, in a small radius around the San Francisco Bay Area. It’s an American phenomenon. You’re not going to win the war with software in AI. It can increase your lethality. At the end of the day, you’re going to have to be able to build things. You’re going to need mass that you can bring to a fight that drives deterrence. And I think the analogy I would give you is World War two. The Germans were far better engineers than the Americans. They had really exquisite technology. They just couldn’t build it at any scale. They could build one off little things here or there. What did we have. We had just an incredible manufacturing capability. It even brought all to Stalin. He couldn’t believe what we were capable of doing. Our things were less sophisticated. But quantity is a quality of its own. And I think if you look at the present moment, you might say that sounds an awful lot like our adversary. And so we need a high-low mixed. And I think part of this is we have to be able to build these things here again with speed. And that’s what the new entrants are really bringing. I don’t think most people recognize that Gulf War one was a watershed moment for the Chinese. They could not believe how quickly the Americans rolled over the fourth largest army in the world. And since that happened, they have been systematically investing in countering the key capabilities that enable us to do that, whether it’s in space, whether it’s our Navy. And so now they’ve had 30 years of thinking about this, developing things that continue to push us back and make our life harder while we haven’t really been treating them like a peer adversary. So I think we’re not behind. But we could be behind if we continue to believe that we have no competition.  

But it’s more than that. You’re not just trying to build mid-level military technology at scale, you’re also trying to create new military technology that integrates AI, especially. What is Titan.  

Titan is a project that we have with the US Army partnered with the prime on it, but we’re partnered with Anduril and l-3 and Northrop Grumman. It’s a satellite ground station on wheels is how to think about it. It’s a truck. There are multiple form factors. There’s a truck version of it. But the whole point is to be able to connect soldiers on the ground to national level intelligence. That allows you to do deep sensing that enables long range precision fires.  

What is deep sensing. How can I sense deep into the battlespace? So the deep sensing is not just what can I see from my truck, but how can I use all the sensors that we have to tell the truck. What can you see. So that is an example of a new form of military technology that did not exist before, that is being designed using the kind of software that I think it becomes feasible.  

Yeah, I mean, many of the concepts have been around for a while, but what is the unit cost of producing it. How long will it take. How many people you need to do it to actually run the software that’s in the truck. Being able to collapse all of that down to something that’s survivable, that’s mobile what would you have done before the truck. You can ask that question. Well, there’s a question why is it even a truck to begin with. Because you need to be mobile. Because if you are static if you have a big command center, you’re going to get blown up. That’s not survivable. US service members will die. Now, if you can do that same thing, that would have taken 400 people in a huge logistical footprint with four people while moving. Hopefully they’re going to survive.  

And how much can you do without people. In a world where part of the future of warfare is drone technology.  

I’m structurally skeptical there. That’s where the fantasy goes. Fantasy terror, fear. Yeah, fear and fantasy both.  

Go on. Tell me your skepticism. That actually, there’s a fair amount of human judgment required for any of these things. It’s not a simple game constrained by rules, and that the better way to think about this is what these soldiers need what our uniformed service members need is an Iron Man suit, something that gives them much more efficiency, much more effectiveness at doing the job that they’re already trained to do. But it’s too hard to get done right now. Or if we did it, the old way would not be survivable in a future conflict and therefore provides no deterrence. Everything’s not about getting into the next fight. It’s actually building technologies that dissuade the next fight.  

But why do you need the person inside the Iron Man suit. If you could just build the Iron Man robot that has the same capacities, I’m not sure you can. First of all, I’m leaving aside just military doctrine that there’s always going to be human on the loop. That’s just right. How the military makes. Well, that’s a related question. Since we’ve been talking about the kill chain, so much of the kill chain assumes that you have to have a human being in the loop when you decide to fire a missile or kill someone. That’s the moral side of this. But you’re arguing that just practically you can’t imagine a world where that efficacious. I don’t think it would work the way that people think it would work terminator nightmare scenario. I’ll give you one example. It’s like I also think there’s an element of this where it’s more of a difference of degree than. So we tend to think about it as O.K, we have these autonomous weapons. They’re going to be built. Well, if we go back to the 70s, we have fighter Jets with radar that see beyond the horizon, see things that the human cannot see that tell you, hey, there’s an enemy aircraft here. So you’re already relying on the computer. There’s some belief in the computer seeing that. Then, assuming you want to engage it, you press a button that releases a missile, or the terminal guidance to the target is done by a computer as well. So that kind of sounds like an autonomous weapon. Sounds like we’ve had autonomous weapons since the 70s. There is a kind of sensationalism around the present moment that makes it sound like, Hey, this is a difference. And I think we’re just saying, I’m saying it’s a difference of degree, actually. And that in the context of having a human on the loop, we’re just extending their capabilities. But there are lots of people, maybe not at Palantir, but in Silicon Valley who think that they are building machines that will be, in some period of time, better than the pilot at making the decision required of them of when and how to pull the trigger. And there’s a level of superintelligence that’s possible through AI. We are racing the Chinese to achieve some level. And that level the atomic bomb, gives you this kind of super advantage in military conflict. And in the end, that’s much more of the whole ballgame game, then figuring out, the right way to improve our kill chain loops. Now I’m super skeptical or skeptical. Tell me, tell me why. Tell me why you’re so skeptical. Because I think the secularists in Silicon Valley are filling the God shaped hole in their heart with AGI, that there’s not an empirical basis to believe that such a thing, it’s like, O.K, the models get better. Why do you think that this cliff is going to happen. Or they somehow turn us into housecats and you kind of see it. It’s like the people who have religion are the most skeptical of this, the people who are kind of transhumanists. And it becomes like what they wish were true. And then they run around with the doomerism, the doomerism of Silicon Valley. It’s both a fundraising shtick where the frontier labs can say, my technology is so powerful, it’s going to lead to mass unemployment. So you better invest in me or you’re going to be poor. And also, it’s divorced from any reality. They are sitting in Silicon Valley building these models, spending no time on the front lines of how are people using this. That’s exactly my job. Being with the ICU nurse at Tampa or being with the submarine industrial parts manufacturer in New Hampshire. Well, when you see it intersect the front line, it’s actually empowering the worker to do more. The nurse spends less time collating clinical notes, more time by the bedside of the patient. The submarine parts manufacturer spends less time looking and analyzing change orders and replanning the work, and more time building the parts that actually move the enterprise forward. So there’s a much more subtle reality that I think is positive and empowering. I think that the ultimate litmus test is when you ask these frontline workers, not only, hey, how do you feel about your job now that you’re using AI. It’s obviously positive, as you might expect from my description here. Much more profoundly, when you ask them, how do you about your children’s future in America, given AI, they’re wildly positive.  

And so how do you square that optimism for the future with the doomerism coming out of the Valley.  

Well how I square it. Which is they’re living in a bubble and it happens to be very good for them from a fundraising perspective. But I think it has huge may be good for you if they are, if it does actually push I further and make like if I want the models to get more powerful, that’s great. I think but the analogy I give you, it’s like electricity just producing electricity is not valuable. We just add have to actually be able to consume it. So the valuable part where the people who built the machine, tools that ran on electricity, that powered the Industrial age, of course, producing electricity is a precondition to that. But we’re so skewed to one side of this equation, the supply side. We’re not thinking about the demand side in a way that’s productive. This is where I do worry about China, because I don’t think they have the same AGI fantasies we do. They’re kind of explicit strategy is to be the best at implementing AI for economic value. That’s what we should be doing.  

So I’m incredibly sympathetic to that narrative. But then I have children, I am religious, I’m not a technologist. So there’s limits to my own understanding of AI. So I always worry that I’m overly tempted to under index on the strong AGI scenarios. And I guess I worry a little bit that you might be tempted for a slightly different reason. Which is that as I keep pressing on, Palantir is in this position of having this incredibly powerful technology that you have to keep making moral and political judgments about. Who do you sell it to who do you work with who do you trust with this technology. And it’s in your interest, in a way, to convince yourself that the moral stakes of your work is always going to be somewhat constrained. You’re not worried that you or anyone else is in the position of building Skynet, inet. Building the system that yields true disaster explicitly not building Skynet. The thing I’d say is I’d offer a little bit of evidence, which is for the. For the frontier labs to be successful, they’ve all started to have to build software around their models. You can think about ChatGPT as the software interface to a model. You can think of all the coding assistants as software interfaces to the model. And I think in the general public, there’s a lack of clarity of what is the software. And what is the AI. And the more and more advanced uses of this stuff really require better models. Sure but they require more software. So I Yes. Maybe you could say no conflict, no interest. The facile view of this would be like, well, you’re incentivized to believe it, but I spent every day building things that inform this position.  

Yeah no, no, I’m. I’m more likely to trust your sense of things than my own. That’s why I’m interviewing you. And that brings me to my last question for you, which is that in addition to being an officer in the U.S. military and the CTO of Palantir, you’re now a would be Hollywood executive, right.  

That’s right. I’ve started a film production company to make content that makes you proud to be an American, not Pravda entertainment. But a lot of this was built on my own assimilation journey. So when I came to the U.S. in the early 80s, I would sit on the couch with dad and watch Rambo 3 and Rocky IV and hunt for Red October and war games and Terminator 2. Just throwing those out there, right. Those two Star Trek. I’ll offer you some optimistic sci-fi. But also things that reflect comedy Beverly Hills Cop. There’s a sense of heroism. It’s not just about war movies. It’s about what is the American spirit. And I like to say, as a five-year-old, I knew what it felt like to be an American before I knew civics or our political philosophy or our history. And I think that feeling is really important. Explains why people went back to the theater to see Top Gun Maverick over and over again. Obviously, they already knew the plot the second time around, but the feeling is really important. And a lot of our content right now is kind of filled with a little bit of self-loathing, maybe a lot of nihilism, a sense that America is a force for bad in the world. Sometimes it’s subtle, sometimes it’s not so subtle. We’ve been here before as a country in Vietnam, essentially. We had a lot of movies that felt this character arc of self-loathing. And it’s actually the reason that George Lucas made American Graffiti in 73, which was kind of tired of it. He just wanted to make a movie about boys chasing girls driving cars. And I think it was kind of the palate cleanser that set the tone for the big pump up movies of the late 70s and 80s that we’ve just been talking about here. And I think they’re really powerful stories to be told. There’s a great opportunity to tell a story on the 25th anniversary of 9/11. That has nothing to do with terrorism or planes flying to the building per se to focus on a very what I view as a quintessentially American story. The 9 everyday New Yorkers who went to work that day and courage was courageous. They motivated each other to get 12,000 souls out of that building. In the 102 minutes between the first strike and the towers collapsing. All of our narratives of this tend to focus on the 2000 or so folks we lost, which is a grave tragedy. But even in our worst moment, that’s the best of us. What is it that we want to communicate to our children that being American means us. I think that’s really important.  

All right. On that patriotic note, Shyam Sankar, Thanks so much for joining me.  

Thanks for having me, Ross.  

What Palantir Sees  
The tech company’s C.T.O. on surveillance, A.I. and the future of war  
October 30, 2025  

